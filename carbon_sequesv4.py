# -*- coding: utf-8 -*-
"""carbon_sequesv4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JoipB5DQxuYRCOiTfaKgH9OO-nhIbyV1
"""

import pandas as pd 
import matplotlib.pyplot as plt 
import seaborn as sns 
import datetime
import numpy as np
import random
from tensorflow import keras
from keras.models import Input, Model, Sequential
from keras.layers import Dense, Dropout, LSTM, Concatenate, SimpleRNN, Masking, Flatten
from keras import losses
from keras.callbacks import EarlyStopping
from keras.initializers import RandomNormal

# ! gdown --id 1g09XRbWd5a5VME5HPdCrAKI9Z6q7RNjd

df = pd.read_csv('pivot_all_clean_training_mod.csv')
df = df.rename(columns=lambda x: x.strip())

df.isna().sum().sum()

# df= df.drop('loc', axis=1)

from datetime import datetime, date, timedelta
df.insert(11, "date_", 0)

for i in range(len(df)) :
  day_num = str(round(df['time'][i]))
  
  day_num.rjust(3 + len(day_num), '0')

  strt_date = date(1970, 1, 1)
    
  res_date = strt_date + timedelta(days=int(day_num))
  res = res_date.strftime("%m-%d-%Y")
  df['date_'][i]= res

df['date_']= pd.to_datetime(df['date_'])

df.set_index('date_', inplace=True)
df.head(2)

features = pd.DataFrame()
features= df
features= df.drop(['time'], axis=1)

features.tail(2)

#features = features.drop(features.columns[71:1037322],axis=1)

features.shape[1]

#features.drop([col for col in features.columns if 'l' in col],axis=1,inplace=True)

features.ffill(axis = 0)

#features = features.drop(features.columns[550:877734],axis=1)

features = features.dropna(axis=1)

features.shape[1]

features.isna().sum().sum()

features.tail(2)

# Number of lags (hours back) to use for models
lag = 15

# Steps ahead to forecast 
n_ahead = 15

# Share of obs in testing 
test_share = 0.25

# Subseting only the needed columns 
ts = features

from sklearn.preprocessing import StandardScaler
scaled_ts = StandardScaler().fit_transform(ts.values)
scaled_ts_df = pd.DataFrame(scaled_ts, index=ts.index, columns=ts.columns)

print(scaled_ts_df.isna().sum().sum())

# Creating the final scaled frame 
ts_s = scaled_ts_df

scaled_ts.shape[1]

ts_s.tail(2)

ts_s.isna().sum().sum()

ts_s.shape[0]

def create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index= list(range(0,1037322,13))) -> tuple:
    """
    A method to create X and Y matrix from a time series array for the training of 
    deep learning models 
    """
    # Extracting the number of features that are passed from the array 
    n_features = ts.shape[1]
    
    # Creating placeholder lists
    X, Y = [], []

    if len(ts) - lag <= 0:
        X.append(ts)
    else:
        for i in range(len(ts) - lag - n_ahead):
            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])
            X.append(ts[i:(i + lag)])
    print(np.count_nonzero(np.isnan(X)))
    print(np.count_nonzero(np.isnan(Y)))       
    X, Y = np.array(X), np.array(Y)
    
    # Reshaping the X array to an RNN input shape 
    X = np.reshape(X, (X.shape[0], lag, n_features))

    return X, Y

X, Y = create_X_Y(ts_s.values, lag=lag, n_ahead=n_ahead)

n_ft = X.shape[2]

np.count_nonzero(np.isnan(X))

Y.shape[2]

# Spliting into train and test sets 
Xtrain, Ytrain = X[0:int(X.shape[0] * (1 - test_share))], Y[0:int(X.shape[0] * (1 - test_share))]
Xval, Yval = X[int(X.shape[0] * (1 - test_share)):], Y[int(X.shape[0] * (1 - test_share)):]

print(f"Shape of training data: {Xtrain.shape}")
print(f"Shape of the target data: {Ytrain.shape}")

print(f"Shape of validation data: {Xval.shape}")
print(f"Shape of the validation target data: {Yval.shape}")

np.count_nonzero(np.isnan(Xtrain))

def plotting(history):
    plt.plot(history.history['loss'], color = "red")
    plt.plot(history.history['val_loss'], color = "blue")
    red_patch = mpatches.Patch(color='red', label='Training')
    blue_patch = mpatches.Patch(color='blue', label='Val')
    plt.legend(handles=[red_patch, blue_patch])
    plt.xlabel('Epochs')
    plt.ylabel('MSE loss')
    plt.show()
    plt.savefig('Train_Vs_Test Loss')

from keras.layers import LSTM, TimeDistributed
import matplotlib.patches as mpatches
model=Sequential()
dim_in = 1037322
dim_out = 79794
nb_units = 10

model.add(LSTM(input_shape=(None, dim_in),
                    return_sequences=True, 
                    units=nb_units))
model.add(TimeDistributed(Dense(activation='linear', units=dim_out)))
model.compile(loss = 'mse', optimizer = 'Adam')

# Training

np.random.seed(1337)
history = model.fit(Xtrain, Ytrain, epochs = 1000, batch_size = 16,
                    validation_data=(Xval, Yval))
plotting(history)

ab= np.array(ts_s[21:37])

ab= np.reshape(ab,(1,15,1037322))

forecast1 = model.predict(ab)

forecast1.shape[2]

ab.shape[1]

ab= np.reshape(ab,(1,15,1037322))

forecast1.shape[0]

forecast1.shape[1]

forecast1.shape[2]

# from sklearn.metrics import r2_score, mean_absolute_error
# r2 = r2_score(Yval[0,1,:], forecast[0,1,:])
# print(r2)
# mae= mean_absolute_error(Yval[0,14,:], forecast[0,14,:])
# print(mae)

scaling= pd.read_csv('big_clean_projection.csv')

scaling.head()

# scaling.insert(4, "date_", 0)
# for i in range(len(scaling)) :
#   day_num = str(round(scaling['time'][i]))
  
#   day_num.rjust(3 + len(day_num), '0')

#   strt_date = date(1970, 1, 1)
    
#   res_date = strt_date + timedelta(days=int(day_num))
#   res = res_date.strftime("%m-%d-%Y")
#   scaling['date_'][i]= res

scaling.describe()

# scaling['loc'] = list(zip(scaling.lat, scaling.lon))

sc= scaling.avgAGC.to_numpy()

sc.shape[0]

sc= np.reshape(sc, (85,79794))

sc1= sc[0,:]

sc1= np.reshape(sc1, (1,79794))

sc1.max()

sc1.min()

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(sc1)
scaled = scaler.transform(sc1)

inversed01 = scaler.inverse_transform(forecast1[0,0,:])
inversed04 = scaler.inverse_transform(forecast1[0,4,:])
inversed10 = scaler.inverse_transform(forecast1[0,9,:])
inversed014 = scaler.inverse_transform(forecast1[0,14,:])
inversed02 = scaler.inverse_transform(forecast1[0,1,:])

#! gdown --id 1jVJc_R1MGbGC-iW0JASXcicky3V8r_ex


1- 453699140.0
2- 453701540.0
3- 453701340.0
5- 453698980.0

(453701540.0- 453699140.0)/ 453701540.0

inversed02.sum()

inversed04.sum()

inversed10.sum()

inversed013 = scaler.inverse_transform(forecast1[0,13,:])

inversed013.sum()

inversed014.sum()

x= [2, 5, 10, 15]
y= [0.000454045383467114, 0.000528984912777221, 0.000484902836712452, -0.0000352656608518147]

plt.plot(x,y, 'r')
plt.xlabel('No of years')
plt.ylabel('Change in AGC values over years in percent')

coor1= pd.read_csv('coor.csv')

import plotly.express as px

fig = px.scatter_mapbox(coor1, lat='lat', lon='lon', color=inversed01, color_continuous_scale='Rainbow_r',opacity = 0.5, size_max= 2, zoom= 6)
fig.update_layout(mapbox_style="stamen-terrain")
fig.update_layout(margin=dict(b=0, t=0, l=0, r=0))

fig.show()

import plotly.express as px

fig = px.scatter_mapbox(coor1, lat='lat', lon='lon', color=inversed05, color_continuous_scale= 'Rainbow_r',
                        opacity = 0.5)
fig.update_layout(mapbox_style="stamen-terrain")
fig.update_layout(margin=dict(b=0, t=0, l=0, r=0))

fig.show()

import plotly.express as px

fig = px.scatter_mapbox(coor1, lat='lat', lon='lon', color=inversed09, color_continuous_scale= 'Rainbow_r',opacity = 0.5)
fig.update_layout(mapbox_style="stamen-terrain")
fig.update_layout(margin=dict(b=0, t=0, l=0, r=0))

fig.show()

import plotly.express as px

fig = px.scatter_mapbox(coor1, lat='lat', lon='lon', color=inversed014, color_continuous_scale= 'Rainbow_r',opacity = 0.5)
fig.update_layout(mapbox_style="stamen-terrain")
fig.update_layout(margin=dict(b=0, t=0, l=0, r=0))

fig.show()
